# -*- coding: utf-8 -*-
"""Text Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g21GTUQeMSJBUQ6Q189730YSZPdul6dH
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("rezkyyayang/reviews-of-indonesian-app-startups-on-playstore")

print("Path to dataset files:", path)

import pandas as pd

# Read the CSV file
df_gojek = pd.read_csv('/root/.cache/kagglehub/datasets/rezkyyayang/reviews-of-indonesian-app-startups-on-playstore/versions/7/gojek.csv')

# Display the first few rows of the dataframe
df_gojek.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Analisis deskriptif untuk kolom 'score'
score_counts = df_gojek['score'].value_counts().sort_index()
plt.figure(figsize=(10, 6))
sns.barplot(x=score_counts.index, y=score_counts.values, palette='viridis')
plt.title('Distribution of Scores')
plt.xlabel('Score')
plt.ylabel('Count')
plt.show()

# Analisis deskriptif untuk kolom 'created_at'
df_gojek['created_at'] = pd.to_datetime(df_gojek['created_at'])
df_gojek['year'] = df_gojek['created_at'].dt.year
year_counts = df_gojek['year'].value_counts().sort_index()
plt.figure(figsize=(12, 6))
sns.lineplot(x=year_counts.index, y=year_counts.values, marker='o', color='green')
plt.title('Number of Reviews per Year')
plt.xlabel('Year')
plt.ylabel('Number of Reviews')
plt.show()

# Cross tabulasi antara 'year' dan 'score'
plt.figure(figsize=(12, 6))
sns.countplot(x='year', hue='score', data=df_gojek, palette='viridis')
plt.title('Score Distribution by Year')
plt.xlabel('Year')
plt.ylabel('Count')
plt.legend(title='Score')
plt.show()

df_gojek_2024 = df_gojek[df_gojek['year'] == 2024]
df_gojek_2024

# Use the .head() method to get the first 100 rows
df_gojek_2024_head = df_gojek_2024.head(100)

# Now df_gojek_2024_head contains only the first 100 rows
df_gojek_2024_head

"""TEXT PREPROCESSING"""

#Import library yang dibutuhkan
import csv
import nltk
import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemover, ArrayDictionary
import string

def filtering_text(text):
    # Check if text is a list and join it into a string
    if isinstance(text, list):
        text = ' '.join(text)
    # mengubah tweet menjadi huruf kecil
    text = text.lower()
    # menghilangkan url
    text = re.sub(r'https?:\/\/\S+','',text)
    # menghilangkan mention, link, hastag
    text = ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ", text).split())
    #menghilangkan karakter byte (b')
    text = re.sub(r'(b\'{1,2})',"", text)
    # menghilangkan yang bukan huruf
    text = re.sub('[^a-zA-Z]', ' ', text)
    # menghilangkan digit angka
    text = re.sub(r'\d+', '', text)
    #menghilangkan tanda baca
    text = text.translate(str.maketrans("","",string.punctuation))
    return text

df_gojek_2024_head['content'] = df_gojek_2024_head['content'].astype(str)

df_gojek_2024_head['filtered'] = df_gojek_2024_head['content'].apply(filtering_text)
print(df_gojek_2024_head['filtered'].head())

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Indonesian stopwords
indonesian_stopwords = stopwords.words('indonesian')

# Create a custom stopword remover
factory = StemmerFactory()
stemmer = factory.create_stemmer()
stopword_factory = ArrayDictionary(indonesian_stopwords)
stopword = StopWordRemover(stopword_factory)

def remove_stopwords(text):
    text = stopword.remove(text)
    return text

df_gojek_2024_head['no_stopwords'] = df_gojek_2024_head['filtered'].apply(remove_stopwords)
print(df_gojek_2024_head['no_stopwords'].head())

import nltk

# Download the 'punkt' and 'punkt_tab' resources
nltk.download('punkt')
nltk.download('punkt_tab')

def word_tokenize_wrapper(text):
    return word_tokenize(text)

df_gojek_2024_head['tokenized'] = df_gojek_2024_head['no_stopwords'].apply(word_tokenize_wrapper)
print(df_gojek_2024_head['tokenized'].head())

# stemming
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(text_cleaning):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    do = []
    for w in text_cleaning:
        dt = stemmer.stem(w)
        do.append(dt)
    d_clean = []
    d_clean = " ".join(do)
    print(d_clean)
    return d_clean

df_gojek_2024_head['stemming'] = df_gojek_2024_head['tokenized'].apply(stemming)

from deep_translator import GoogleTranslator

# Menerjemahkan setiap kata dalam setiap entri kolom 'full_text' ke dalam bahasa Inggris
translated_tweets = []
for tweet in df_gojek_2024_head['stemming']:
    translated_words = []
    for word in tweet.split():
        try:
            translated_word = GoogleTranslator(source='id', target='en').translate(word)
        except TranslationNotFound:
            # If translation not found, keep the original word
            translated_word = word
            print(f"Translation not found for: {word}, using original word.")
        translated_words.append(translated_word)
    translated_tweet = ' '.join(translated_words)
    translated_tweets.append(translated_tweet)

# Menambahkan kolom baru 'tweet_english' ke dalam data
df_gojek_2024_head['translated'] = translated_tweets

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

def analyze_sentiment(text):
    scores = analyzer.polarity_scores(text)
    return scores['compound']

df_gojek_2024_head['vader_sentiment'] = df_gojek_2024_head['translated'].apply(analyze_sentiment)

# Function to label sentiment based on compound score
def label_sentiment(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df_gojek_2024_head['sentiment_label'] = df_gojek_2024_head['vader_sentiment'].apply(label_sentiment)

df_gojek_2024_head[['translated', 'vader_sentiment', 'sentiment_label']].head()

# Menghitung jumlah masing-masing label
label_counts = df_gojek_2024_head['sentiment_label'].value_counts()

label_counts

"""MULTINOMIAL NAIVE BAYES"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Pisahkan data menjadi fitur (teks) dan target (label sentimen)
X = df_gojek_2024_head['translated']
y = df_gojek_2024_head['sentiment_label']

# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Buat objek TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit dan transform data latih
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

# Transform data uji
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Buat dan latih model Multinomial Naive Bayes
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# Lakukan prediksi pada data uji
y_pred = nb_classifier.predict(X_test_tfidf)

# Evaluasi model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

# Membuat confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['negative', 'neutral', 'positive'],
            yticklabels=['negative', 'neutral', 'positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""LATENT DIRICHLET ALLOCATION"""

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords  # Import stopwords from NLTK

# Get Indonesian stopwords from NLTK
indonesian_stopwords = stopwords.words('indonesian')

# Create a CountVectorizer with Indonesian stopwords
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=indonesian_stopwords)
dtm = vectorizer.fit_transform(df_gojek_2024_head['stemming'])

# Create a LatentDirichletAllocation model
LDA = LatentDirichletAllocation(n_components=5, random_state=42) # You can adjust n_components

# Fit the model to the document-term matrix
LDA.fit(dtm)

# Print the top words for each topic
for index, topic in enumerate(LDA.components_):
    print(f"Top 15 words for Topic #{index}")
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-15:]])
    print("\n")